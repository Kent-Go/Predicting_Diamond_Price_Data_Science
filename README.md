# Predicting_Diamond_Price_Data_Science

## Phase 1: Data Preparation & Visualisation
We defined the objective of our report, which is predicting the diamond's price in US dollars based on the available features provided in the diamond's dataset. Subsequently, we conducted data cleaning process by removing outliers which has a higher and lower carat than 1.5 times of the interquantile range, and identified if there any missing values occurs within the dataset. However, the dataset is complete thus not containing any missing value. Eventually, we visualised and explored the dataset using 15 figures that plot and illustrate the relationship between different variables using Seaborn and Matplotlib. An explanation is provided for each plot to analyse the patterns, correlations, and trends occured. Finally, comprehensive summary and conclusion are written to demonstrate the insights gained from Phase 1, and establish its connections with our objectives. References are added at the end to cite any dataset and information used in APA format as required.
Informations and analysis obtained from Phase 1 would contribute to further investigation on this multiple linear regression problem using statistical modelling to evaluate the relationship between a predictor variable and the response variable (price of diamond) while controlling the potential influence of other diamond's variables. This would assist in predicting the diamond's price strategically using its different variables in Phase 2.

## Phase 2: Statistical Modelling
We apply statistical modelling onto diamonds' variables. In order to forecast the price of diamonds using all of the available features, we first fit a multiple linear regression in the Full Model Overview. We firstly rename a few of the column names in our Full Model Overview because we did not do so in our Data Preprocessing and Cleaning section in our Phase 1 report. Next, we use the Statsmodels module to create the regression formula as a Python string that contains all of the independent variables from our dataset. The categorical features in our dataset are then one-hot encoded using Pandas' get_dummies function, and the regression model formula is updated to include the encoded features.

After defining the regression model formula, we create an ordinary least squares (OLS) model to the encoded features. The OLS regression results table contains numerous informations about the dataset, such as the dependent variable, number of observations, degree of freedom of residuals, the R-squared and Adjusted R-squared value, F-statistics, the coefficient term, standard error parameters, t-statistics, p-values, and confidence intervals. In order to better comprehend the difference between the real and predicted prices of diamonds, we then establish a new data frame to hold the actual price of diamonds, the predicted price of diamonds, as well as its residuals.

Next, we check whether the multiple linear regression model is valid or not by running diagnostic checks with 4 assumptions to satifsy, which are linearity, nearly normal residuals, constant variability, and independence of residuals. In backwards feature selection, we eliminate features that are not statistically significant by looking that features whose p-value is above 0.05 since statistical significance is indicated by a p-value of less than 0.05. We create a new OLS model and data frame that stores the actual price of diamonds, the predicted price of diamonds, and its residuals as well as to validate whether the model satisfies the previous 4 assumptions in our Reduced Model Overview and Reduced Model Diagnostic Checks. This is similar to what we did in the Full Model Overview and Full Model Diagnostic Checks, but with the insignificant features eliminated.
